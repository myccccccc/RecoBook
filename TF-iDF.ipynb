{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords #Manually download\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from util import rmse\n",
    "from util import ndcg_k\n",
    "from util import divSco_k\n",
    "from util import gettail\n",
    "\n",
    "MIN_WORD_COUNT = 15\n",
    "# #MODEL = \"GaussianNB\"\n",
    "# MODEL = \"MultinomialNB\"\n",
    "\n",
    "reallyfinalbooks = pd.read_csv('reallyfinalbooks.csv')\n",
    "reallyfinalratings = pd.read_csv('reallyfinalratings.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "traintable = pd.pivot_table(data=train, index=\"newuser_id\", columns=\"newbook_id\", values=\"rating\").sort_index(axis=0).sort_index(axis=1)\n",
    "testtable = pd.pivot_table(data=test, index=\"newuser_id\", columns=\"newbook_id\", values=\"rating\").sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "tail = gettail(reallyfinalratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reallyfinalbooks[\"description_list\"] = (reallyfinalbooks[\"description\"]\n",
    "                                        .replace(\"Unknown\", np.NaN)\n",
    "                                        .fillna(reallyfinalbooks[\"title_without_series\"])\n",
    "                                        .str.replace(r'[^\\w\\s\\']',\"\")\n",
    "                                        .str.lower()\n",
    "                                        .str.split())\n",
    "reallyfinalbooks[\"description_list\"] = (reallyfinalbooks[\"description_list\"]\n",
    "                                        + (reallyfinalbooks[\"genre\"]\n",
    "                                           .str.replace(\"|\", \" \")\n",
    "                                           .str.replace(\"-\", \" \")\n",
    "                                           .str.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = []\n",
    "for l in reallyfinalbooks[\"description_list\"]:\n",
    "    allwords += list(set(l))\n",
    "wordcounts = pd.DataFrame({\"count\": allwords})[\"count\"].value_counts().to_frame().sort_values(by=\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordbag = []\n",
    "for word in wordcounts.index:\n",
    "    if wordcounts.loc[word, \"count\"] >= MIN_WORD_COUNT and word not in stopwords.words(\"english\") and len(word) > 1:\n",
    "        wordbag.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iDFs = []\n",
    "for word in wordbag:\n",
    "    iDF = np.log(len(reallyfinalbooks) / sum(reallyfinalbooks[\"description_list\"].apply(lambda l: 1 if word in l else 0)))\n",
    "    iDFs.append(iDF)\n",
    "word_iDFs = pd.DataFrame({\"word\":wordbag, \"iDF\":iDFs}).set_index(\"word\").sort_values(by=\"iDF\")\n",
    "word_iDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def getTF_iDF(descriptionlist, word_iDF):\n",
    "    wordlist = list(filter(lambda x: x in word_iDFs.index, descriptionlist))\n",
    "    wordcounter = collections.Counter(wordlist)\n",
    "    TF_iDF_vector = list(map(lambda x: wordcounter[x] * word_iDFs.loc[x, \"iDF\"] / len(wordlist), word_iDFs.index))\n",
    "    return TF_iDF_vector\n",
    "\n",
    "reallyfinalbooks[\"TF-iDF_vector\"] = reallyfinalbooks[\"description_list\"].apply(lambda x: getTF_iDF(x, word_iDFs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def getCosine_Similarity(v1, v2):\n",
    "    numerator = v1 @ v2\n",
    "    denominator = np.sqrt(np.sum(np.square(v1), axis=-1)) * np.sqrt(np.sum(np.square(v2), axis=-1))\n",
    "    return numerator / denominator\n",
    "\n",
    "v1 = np.stack(reallyfinalbooks[\"TF-iDF_vector\"])\n",
    "bookid2similarities = {}\n",
    "\n",
    "for bookid, v2 in zip(reallyfinalbooks[\"newbook_id\"], reallyfinalbooks[\"TF-iDF_vector\"]):\n",
    "    bookid2similarities[bookid] = getCosine_Similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "similarity_table = pd.DataFrame.from_dict(bookid2similarities, orient=\"index\", columns=reallyfinalbooks[\"newbook_id\"])\n",
    "bookid2similarities = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_iDFModel:\n",
    "    \n",
    "    def __init__(self, similarity_table):\n",
    "        self.similarity_table = similarity_table\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "            X: A list of training data bookids\n",
    "            y: A list of training data ratings\n",
    "        \"\"\"\n",
    "        self.traindatasimilarity = self.similarity_table.loc[X,:]\n",
    "        self.traindatarating = np.array(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "            X: A list of bookids waiting to be predicted\n",
    "        Returns:\n",
    "            Predicted ratings\n",
    "        \"\"\"\n",
    "        predictdatasimilarity = self.traindatasimilarity.loc[:, X]\n",
    "        total_weights = np.sum(predictdatasimilarity, axis=0)\n",
    "        return (self.traindatarating @ predictdatasimilarity) / total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "userid2preds = {}\n",
    "userid2pred_ratings = {}\n",
    "userids = list(reallyfinalratings[\"newuser_id\"].unique())\n",
    "\n",
    "\n",
    "for counter, userid in enumerate(userids):\n",
    "    if (counter % 500 == 0):\n",
    "        print(counter)\n",
    "    userid2usertraindata = train[train[\"newuser_id\"] == userid]\n",
    "    m = TF_iDFModel(similarity_table)\n",
    "    m.fit(userid2usertraindata[\"newbook_id\"], userid2usertraindata[\"rating\"])\n",
    "    pred =  (reallyfinalbooks[[\"newbook_id\"]]\n",
    "            .assign(pred_rating=m.predict(reallyfinalbooks[\"newbook_id\"]))\n",
    "            .sort_values(by=\"pred_rating\", ascending=False)\n",
    "            .assign(rank=np.arange(1, len(reallyfinalbooks)+1))\n",
    "            .sort_values(by=\"newbook_id\")\n",
    "            .set_index(\"newbook_id\"))\n",
    "    userid2preds[userid] = pred\n",
    "    userid2pred_ratings[userid] = pred[\"pred_rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_table = pd.DataFrame.from_dict(userid2pred_ratings, orient=\"index\", columns=range(1, len(reallyfinalbooks)+1))\n",
    "pred_table = pred_table.sort_index(axis=0).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_rmse = rmse(traintable.values, pred_table.values)\n",
    "test_rmse = rmse(testtable.values, pred_table.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "userids = reallyfinalratings[\"newuser_id\"].unique()\n",
    "train_rs = []\n",
    "test_rs = []\n",
    "for userid in userids:\n",
    "    train_rs.append(train[train[\"newuser_id\"] == userid].merge(userid2preds[userid], on=\"newbook_id\", how=\"left\").sort_values(by=\"rank\")[\"rating\"])\n",
    "    test_rs.append(test[test[\"newuser_id\"] == userid].merge(userid2preds[userid], on=\"newbook_id\", how=\"left\").sort_values(by=\"rank\")[\"rating\"])\n",
    "\n",
    "train_ndgc = np.mean([ndcg_k(r) for r in train_rs])\n",
    "test_ndgc = np.mean([ndcg_k(r) for r in test_rs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "diversityScore = np.mean([divSco_k(pred.sort_values(by=\"rank\").index,tail[\"newbook_id\"].values) for pred in list(userid2preds.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Model\n",
      "RMSE for train data: 0.910, test data: 0.917\n",
      "nDGC for train data: 0.616, test data: 0.690\n",
      "Diversity Score: 0.130\n"
     ]
    }
   ],
   "source": [
    "print(\"NB Model\")\n",
    "print(\"RMSE for train data: {:.3f}, test data: {:.3f}\".format(train_rmse, test_rmse))\n",
    "print(\"nDGC for train data: {:.3f}, test data: {:.3f}\".format(train_ndgc, test_ndgc))\n",
    "print(\"Diversity Score: {:.3f}\".format(diversityScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
